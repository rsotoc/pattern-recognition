{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Clasificación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Clasificación%20V.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: Redes Neuronales\n",
    "\n",
    "\n",
    "### Las metáforas en aprendizaje automático\n",
    "**<font style=\"color:#0011d2\">Metáfora</font>**\n",
    "<p style=\"background-color:#e8e8e8; color:#008100\">Del lat. metaphŏra, y este del gr. μεταφορά metaphorá.\n",
    "\n",
    "1. <font style=\"color:#006ec3\">f. *Ret.*</font> Traslación del sentido recto de una voz a otro figurado, en virtud de una comparación tácita, como en las perlas del rocío, la primavera de la vida o refrenar las pasiones. ([RAE](http://dle.rae.es/?id=P4sce2c))<br>\n",
    "\n",
    "Las metáforas son uno de los dispositivos más utilizados como figuras literarias. Una metáfora se refiere a un significado o a la identidad atribuida a un sujeto por medio de otro. ([Figuras Literarias](http://figurasliterarias.org/content/metáfora))<br>\n",
    "\n",
    "![](images/metaphors.jpg)<br>\n",
    "\n",
    "El uso de una metáfora no se entiende como una comparación fiel, en un sentido directo. <br>\n",
    "\n",
    "En el caso de **aprendizaje automático**, se utilizan metáforas con diversos sistemas naturales/sociales para desarrollar sistemas artificiales cuyo funcionamiento recuerda \"*de alguna manera*\" al funcionamiento de sus contrapartes metafóricas.<br>\n",
    "\n",
    "El uso de metáforas para el desarrollo de sistemas artificiales se originó en el trabajo de Arturo Rosenblueth y Norbert Wiener. Su colaboración dió origen a la cibernética y, de ahí, a la inteligencia artificial. Algunas referencias importantes que documentan esta colaboración son:\n",
    "\n",
    "* [Behavior, Purpose and Teleology](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf), 1943; por Arturo Rosenblueth, Norbert Wiener y Julian Bigelow. El documento que inició todo.\n",
    "* [Arturo Rosenblueth y Norbert Wiener: dos científicos en la historiografía de la educación contemporánea](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf)\n",
    "* [Coloquio: Norbert Wiener y Arturo Rosenblueth: un encuentro interdisciplinario](http://computo.ceiich.unam.mx/webceiich/deptoDifusion/32PreMed/12-05-B2.pdf)\n",
    "* [Analogías entre hombre y máquina. El Grupo Cibernética y algunas de sus ideas fundacionales](http://www.con-temporanea.inah.gob.mx/node/42)\n",
    "* [interview: Heinz von Foerster](https://web.stanford.edu/group/SHR/4-2/text/interviewvonf.html)\n",
    "* [Perspectives on the history of the cybernetics movement: The path to current research through the contributions of Norbert Wiener, Warren McCulloch, and John von Neumann](http://www.univie.ac.at/constructivism/archive/fulltexts/3763.html)\n",
    "\n",
    "![](images/rosenblueth-wiener.jpg)<br>\n",
    "\n",
    "El primer resultado importante del uso de metáforas biológicas para el desarrollo de sistemas automatizados fue la Neurona de McCulloch-Pitts que daría origen a las redes neuronales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona biológica\n",
    "\n",
    "La neurona es la célula base del sistema nervioso. Su característica más importante es la excitabilidad eléctrica de su membrana plasmática. Esta peculiaridad le ha permitido desarrollar una especialización en la recepción de estímulos eléctricos y la transmisión de esta información en forma de impulsos eléctricos. Los estímulos pueden provenir de otras neuronas o de otros tipos de células, por ejemplo células sensoriales. El impulso nervioso es envido a otras neuronas o hacia otros tipos de células, por ejemplo, hacia las fibras musculares. <br><br>\n",
    "\n",
    "![](images/Complete_neuron_cell_diagram_es.png)<br>\n",
    "\n",
    "Podemos simplificar la estructura de la neurona de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron0.png)\n",
    "\n",
    "La membrana celular juega un papel muy importante en la funcionalidad de la neurona. Como en todas la células vivas, la membrana tiene una función estructural al separar físicamente el citoplasma y sus componentes intracelulares del ambiente extracelular. \n",
    "\n",
    "<img src=\"images/0312_Animal_Cell_and_Components.jpg\" width=400>\n",
    "\n",
    "La membrana también juega un papel importante en la nutrición de la célula debido a que es selectivamente permeable y, por lo tanto, es capaz de regular lo que entra y sale de la célula. \n",
    "\n",
    "Esta separación de componentes tiene un efecto colateral interesante: dado que los materiales que entran y salen de la célula son típicamente iones y dado que la membrana es selectivamente permeable, es usual encontrar una diferencia de potencial eléctrico dentro y fuera de la célula, lo que se conoce como *potencial en reposo*.\n",
    "\n",
    "<img src=\"images/11-08_RestMemPoten.jpg\" width=600>\n",
    "\n",
    "La característica morfológica más distintiva de una neurona como célula, es el conjunto de \"extensiones\" de su membrana: las dendritas y los axones. Las dendritas tienen la función de recibir los estímulos provenientes de otras neuronas en forma de neurotransmisores químicos y convertirlos en potenciales eléctricos en la membrana.\n",
    "\n",
    "<img src=\"images/synapsis.png\" width=600>\n",
    "\n",
    "<img src=\"images/640px-Neurotransmitters.jpg\" width=600>\n",
    "\n",
    "Cuando el potencial eléctrico en la membrana receptora es suficientemente alto, es decir, cuando se alcanza un cierto umbral de excitación en la membrana, se modifica la permeabilidad de ésta provocando un flujo de iones hacia adentro y afuera de la célula. \n",
    "\n",
    "Este movimiento de iones a través de la membrana genera una corriente eléctrica que a su vez provoca un tren de cambios locales de permeabilidad a lo largo de todo el axón. \n",
    "\n",
    "<img src=\"images/action_potential.jpg\" height=200px>\n",
    "\n",
    "Este tren de *potenciales de acción* constituye el impulso eléctrico que, de esta manera, \"viaja\" desde el cuerpo celular hasta las *sinapsis* en las dendritas terminales en el axón. \n",
    "\n",
    "![](images/neuron1.png)\n",
    "\n",
    "Al final del recorrido, las señales eléctricas provocan la liberación de substancias químicas (los neurotransmisores) que tendrán la función de excitar las dendritas de las neuronas vecinas, logrando de esta manera que la señal viaje a lo largo de un determinado camino por el sistema nervioso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona de McCulloch-Pitts\n",
    "\n",
    "El primer modelo de neurona artificial fue propuesto por [Warren S. McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) (neurocientífico) y [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts) (matemático) en 1943 en el artículo \"[A logical calculus of the ideas immanent in nervous activity](http://www.minicomplexity.org/pubs/1943-mcculloch-pitts-bmb.pdf)\". Esta neurona es denominada **neurona de McCulloch-Pitts**, *neuronas MCP*, o *unidad/compuerta lógica de umbral*.\n",
    "\n",
    "![](images/mcp-neuron.png)\n",
    "\n",
    "\n",
    "Este modelo implementa la metáfora neuronal de la siguiente manera:\n",
    "\n",
    "1. Las señales provenientes del ambiente o de otras neuronas se modelan como el conjunto de entradas $\\{x_1, x_2, \\ldots, x_n\\}$. \n",
    "2. Cada una de las señales de entrada ejerce un estímulo en la neurona que depende de la actividad sináptica, la que a su vez codifica qué tanta importancia da la neurona a una determinada señal. En el modelo, esta importancia se representa mediante el conjunto de *pesos sinápticos* $\\{w_1, w_2, \\ldots, w_n\\}$.\n",
    "3. Los estímulos provenientes de cada entrada (la señal ponderada por el peso), son integrados en el cuerpo celular. Dado que, en este caso, se ignoran los estados preliminares de la neurona, esta suma ponderada representa la activación de la neurona en el tiempo actual. Así, la activación de la neurona $j$ en el tiempo $t$ está dada por: $$a_j(t) = \\sum_{i=1}^n w_i x_i$$\n",
    "4. La salida de la neurona es un proceso \"todo o nada\": si la activación supera un cierto valor umbral $\\theta$ la neurona se activa y transmite un impulso (salida $y=1$), en caso contrario no *dispara*. De esta manera, la salida de la neurona $j$ en el tiempo $t$ está dada por: <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad a_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad a_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "lo cual puede reescribirse como <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "siendo $\\mathbf{w}_j$ y $\\mathbf{x}_j(t)$ el vector de pesos (constante) de la neurona $j$ y el vector de entrada a la neurona en el tiempo $t$.\n",
    "\n",
    "La neurona de McCulloch-Pitts puede, entonces, representarse de la siguiente manera:\n",
    "\n",
    "![](images/neuron2.png)\n",
    "\n",
    "El problema ahora se reduce a escoger adecuadamente los pesos $w_i$.\n",
    "\n",
    "Originalmente, sin un procedimiento adecuado para ajustar los pesos sinápticos, particularmente para neuronas en una red, la neurona de McCulloch-Pitts era apenas una curiosidad y un primer resultado que validaba las posibilidades del uso de metáforas para el desarrollo de sistemas artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de Hebb\n",
    "\n",
    "La [teoría Hebbiana](http://lcn.epfl.ch/~gerstner/PUBLICATIONS/Gerstner-Plasticity2011.pdf) propuesta por Donald Hebb en 1949 ofrece una explicación del aprendizaje en términos de la adaptación de las neuronas durante su actividad. De acuerdo con Hebb, una actividad o señal que se repite regularmente y que tiene un efecto establizador en la célula, tiende a fortalecerse, transformando una memoria de corto plazo en un *engrama*, esto es, una estructura de interconexión neuronal estable.\n",
    "\n",
    "Hebb describe este proceso de la siguiente manera: \n",
    "\n",
    ">When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\n",
    "\n",
    "Es decir, la *importancia* que la neurona B presta a las señales provenientes de la neurona A se intensifica con cada interacción exitosa. Esta teoría recibe también el nombre de Regla de Hebb o teoría de la Sinápsis Hebbiana. \n",
    "\n",
    "Esta regla puede expresarse, matemáticamente de la siguiente manera: Supongamos que $y_k$ es la salida de la neurona postsináptica $k$ dada la salida $x_{i}$ de la neurona presináptica $i$, entonces, si $\\eta$ es la tasa de aprendizaje, la actualización del peso que la neurona $k$ da a la entrada proveniente de la neurona $i$ está dada por:\n",
    "\n",
    "$$\\Delta w_{ki}=\\eta x_{i} y_k$$ \n",
    "\n",
    "![](images/neuron3.png)\n",
    "\n",
    "La regla de Hebb, utilizada para actualizar los pesos de la neurona de McCulloch-Pitts, abre la puerta a sistemas neuronales efectivos.\n",
    "\n",
    "\n",
    "### El perceptrón\n",
    "\n",
    "El **[perceptrón](https://en.wikipedia.org/wiki/Perceptron)** es un modelo de neurona obtenido al combinar el modelo de la neurona de McCulloch-Pitts con una extensión de la regla de Hebb. La regla de entrenamiento consta de los siguientes pasos:\n",
    "\n",
    "1. Inicializar los pesos con valores aleatorios pequeños.\n",
    "2. Para cada vector de entrenamiento $\\mathbf{x}_j$ con valor de clase $y_j$ (el valor de salida esperado): \n",
    "    1. Calcular el valor de salida  $y^*_j$ de la función de activación\n",
    "    2. Actualizar los pesos de acuerdo a la regla de Hebb modificada:<BR>\n",
    "    $$\\Delta \\mathbf{w} = \\eta (y_j - y^*_j) \\mathbf{x}_j$$\n",
    "    \n",
    "\n",
    "A continuación mostramos el algoritmo de entrenamiento del perceptrón. Analizamos el caso de clasificación de los datos de la flor Iris. Dado que el preceptrón tiene sólo dos salidas (0 y 1). sólo podemos clasificar en dos clases por lo que, para este ejercició, utilizaremos sólo los primeros 100 datos del conjunto de datos *Iris Data Set*. Utilizamos los primeros 50 datos (revueltos) para entrenar el perceptrón, imprimimos sólo los casos donde hubo error y la consecuente modificación de los pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar datos de Iris Data Set\n",
    "df = pd.read_csv(\"Data sets/Iris Data Set/iris.data\", header = None)\n",
    "df = shuffle(df.iloc[0:100])\n",
    "\n",
    "# Etiqueta de clase de cada vector ejemplo\n",
    "Y = df.iloc[0:100, 4].values\n",
    "Y = np.where(Y == 'Iris-setosa', 0, 1) # convertir las etiquetas de clase a valores de clase\n",
    "\n",
    "# Vector de características\n",
    "X = df.iloc[0:100, 0:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector\tTarget\toutput\terror\tw\n",
      "2 \t 0 \t 1 \t -1 \t [-0.051 -0.034 -0.015 -0.002]\n",
      "4 \t 1 \t 0 \t 1 \t [-0.002 -0.01   0.018  0.008]\n",
      "Pesos finales:  [-0.002 -0.01   0.018  0.008]\n"
     ]
    }
   ],
   "source": [
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "# Datos para entrenameinto y prueba\n",
    "train_data, test_data, train_y, test_y = train_test_split(X, Y, test_size=.5)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Vector\\tTarget\\toutput\\terror\\tw\")\n",
    "vector = 1\n",
    "for xi, target in zip(train_data, train_y) :\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    w += eta * error * xi\n",
    "    if(error != 0):\n",
    "        print(vector, \"\\t\", target, \"\\t\", output, \"\\t\", error, \"\\t\", w)\n",
    "    vector += 1\n",
    "    \n",
    "print (\"Pesos finales: \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que los valores de los pesos dependerá de los valores iniciales de los pesos y de la secuencia en que se van presentando los vectores de entrenamiento. En la siguiente tabla se presentan los pesos finales para diversas corridas:\n",
    "\n",
    "Corrida | Pesos finales  \n",
    "----| ----|\n",
    "1   | [-0.012 -0.044  0.078  0.035]\n",
    "2   | [-0.022 -0.052  0.076  0.036]\n",
    "3   | [-0.014 -0.057  0.081  0.033]\n",
    "4   | [-0.020 -0.046  0.064  0.027]\n",
    "5   | [-0.017 -0.049  0.074  0.028]\n",
    "\n",
    "Los valores específicos de los pesos carecen de importancia y lo importante es el desempeño del clasificador.\n",
    "\n",
    "A continuación los utilizamos los siguientes 50 vectores para probar la eficacia del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectores mal clasificados de 50 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_data, test_y) :\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_data), \n",
    "                                                        errores/len(test_data)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el experimento, ahora con los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar los datos\n",
    "dfPID = pd.read_csv(\"Data sets/Pima Indian Data Set/pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "# Eliminar datos con valores faltantes\n",
    "dfPID.loc[dfPID['pad'] == 0,'pad'] = np.nan\n",
    "dfPID.loc[dfPID['ept'] == 0,'ept'] = np.nan\n",
    "dfPID.loc[dfPID['is2h'] == 0,'is2h'] = np.nan\n",
    "dfPID.loc[dfPID['imc'] == 0,'imc'] = np.nan\n",
    "dfPID = dfPID.dropna()\n",
    "\n",
    "# Formar vectores de características y normalizar\n",
    "df_pure = dfPID[list(['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad'])]\n",
    "std_data = preprocessing.StandardScaler().fit_transform(df_pure)\n",
    "df_pure = pd.DataFrame(std_data)\n",
    "\n",
    "# Valores de salida\n",
    "df_class = dfPID[list(['class'])].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 vectores mal clasificados de 130 (51.53846153846153%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[43 37]\n",
      " [30 20]]\n"
     ]
    }
   ],
   "source": [
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure.values, df_class, test_size=0.33, random_state=0)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPID = np.zeros(X_trainPID.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "for xi, target in zip(X_trainPID, y_trainPID):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que los resultados para problemas más complejos no es satisfactorio. Este resultado no es realmente una sorpresa dado que una neurona como la planteada es, simplemente, un clasificador lineal. A continuación los resultados de 5 corridas con diferentes particiones de datos:\n",
    "\n",
    "Corrida | Resultados  \n",
    "----| ----|\n",
    "1   | 26 vectores mal clasificados de 79 (32.91%)\n",
    "2   | 31 vectores mal clasificados de 79 (39.24%)\n",
    "3   | 37 vectores mal clasificados de 79 (46.84%)\n",
    "4   | 55 vectores mal clasificados de 79 (69.62%)\n",
    "5   | 18 vectores mal clasificados de 79 (22.78%)\n",
    "\n",
    "Como puede observarse, el porcentaje de error, en las corridas presentadas, va desde 22.78% hasta 69.62%. Esta variabilidad en la capacidad de clasificación es debida a las siguientes características del proceso de entrenamiento del perceptrón:\n",
    "\n",
    "1. El error cometido por el perceptrón durante el entrenamiento, el cual se utiliza para realizar el ajuste de pesos, es de caracter \"todo-o-nada\": Solo se toma en cuenta si la clasificación fue correcta en términos del valor discreto de clase, pero no se toman en cuenta los aciertos que estuvieron cerca de fallar y se magnifican los errores cometidos por un margen pequeño (ambos casos correspondientes a puntos muy cercanos al hiperplano definido por los pesos actuales).\n",
    "2. El perceptrón corrije los pesos cada que encuentra un vector que se clasifica erróneamente, lo que lo hace susceptible a valores atípicos.\n",
    "\n",
    "El resultado es un hiperplano no óptimo, altamente dependiente de la secuencia de presentación de los vectores de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación realizamos una segunda ronda de entrenamiento con los mismos datos de entrenamiento, barajeados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 vectores de entrenamiento mal clasificados de 263 (42.20532319391635%)\n",
      "25 vectores mal clasificados de 130 (19.230769230769234%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[64 16]\n",
      " [ 9 41]]\n"
     ]
    }
   ],
   "source": [
    "# 2a ronda de entrenamiento\n",
    "vectores = 0\n",
    "shuffled_data = shuffle(list(zip(X_trainPID, y_trainPID)))\n",
    "for xi, target in shuffled_data:\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    if (target != output) :\n",
    "        vectores += 1\n",
    "print(\"{} vectores de entrenamiento mal clasificados de {} ({}%)\".\n",
    "      format(vectores, len(y_trainPID), vectores/len(y_trainPID)*100))\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos todos los datos para continuar el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 vectores de entrenamiento mal clasificados de 393 (31.552162849872772%)\n",
      "27 vectores mal clasificados de 130 (20.76923076923077%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[59 21]\n",
      " [ 6 44]]\n"
     ]
    }
   ],
   "source": [
    "# 3a ronda de entrenamiento... con todos los datos\n",
    "vectores = 0\n",
    "for xi, target in zip(df_pure.values, df_class):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    if (target != output) :\n",
    "        vectores += 1\n",
    "print(\"{} vectores de entrenamiento mal clasificados de {} ({}%)\".\n",
    "      format(vectores, len(df_class), vectores/len(df_class)*100))\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado apenas mejora o incluso empeora en diferentes corridas. El plano de clasificación alcanzó su máxima eficacia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline y la regla delta\n",
    "\n",
    "#### El modelo Adaline\n",
    "\n",
    "La siguiente modificación al modelo de neurona se obtiene observando que en la regla de Hebb, la modificación en la neurona A se realiza a partir de la estimulación obtenida debido a la señal proveniente de la neurona B y no al disparo de una respuesta (\"When an axon of cell A is near enough to excite a cell B...\"). Este ajuste da lugar al modelo de neurona Adaline.\n",
    "\n",
    "La **neurona lineal adaptativa** o **Adaline** (*Adaptive Linear Neuron*) puede describirse de acuerdo a la siguiente estructura:\n",
    "\n",
    "![](images/neuron6.png)\n",
    "\n",
    "En esta neurona, la función de activación se divide en una función de activación lineal continua, concretamente la función identidad y una función de salida o cuantificador que es la responsable de generar la salida discreta (la salida es, en muchos casos seleccionada como [-1,1] por conveniencia). De esta manera, el cálculo del error y la actualización de los pesos sinápticos se realiza a partir de la diferencia entre la salida esperada y la suma ponderada de las entradas:<br><br>\n",
    "\n",
    "![](images/neuron7.png)\n",
    "\n",
    "$$error = y_j - \\sum_{i=1}^n w_i x_{ji} = y_j - \\mathbf{w}\\cdot \\mathbf{x}_j(t)$$\n",
    "\n",
    "Esta medida de error es la distancia de cada vector al hiperplano de clasificación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 vectores mal clasificados de 130 (21.53846153846154%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[64 16]\n",
      " [12 38]]\n"
     ]
    }
   ],
   "source": [
    "# Vector de pesos inicial\n",
    "wPID = np.zeros(X_trainPID.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "for xi, target in zip(X_trainPID, y_trainPID):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    error = target - activation\n",
    "    wPID += eta * error * xi\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El sesgo\n",
    "Otra modificación que suele hacerse a la neurona artificial, está inspirada en el clasificador lineal, aunque tiene una contraparte biológica en el potencial de activación o potencial umbral de la neurona. En la siguiente gráfica se muestra el potencial eléctrico de una neurona biológica:\n",
    "\n",
    "![](images/potencial-de-accion.png)\n",
    "\n",
    "Como puede observarse, la neurona siempre mantiene un potencial eléctrico (de reposo), independientemente de que no reciba una entrada y genera un impulso (un potencial de acción), únicamente en aquellos casos en los que la excitación de la membrana sobrepasa un cierto potencial. Esto equivale, en la neurona artificial, a modificar el valor de activación para el cual la neurona artificial arroja un 1, o bien, a introducir un valor de *entrada* independiente de las señales provenientes de las otras neuronas. Este término se denomina *Bias*. \n",
    "\n",
    "Desde una perspectiva del clasificador lineal, observemos que el término $\\mathbf{w}\\cdot \\mathbf{x}_j(t)$ describe una línea que pasa por el origen. Esta condición (de pasar por el origen) restringe la capacidad del clasificador:<br><br>\n",
    "\n",
    "![](images/linear-plane-bias.png)\n",
    "\n",
    "Al agregar un término independiente aumenta la felxibilidad del clasificador. \n",
    "\n",
    "De manera que ahora, la estructura de la neurona artificial puede representarse de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron8.png)\n",
    "\n",
    "Donde el bias se integra al modelo como un \"nodo\" de entrada constante (típicamente con valor 1) y un peso $w_0$ adicional que permite ajustar la posición del hiperplano clasificador. \n",
    "\n",
    "#### La regla delta\n",
    "Otra ventaja de la función de activación lineal en la neurona Adaline, con respecto a la función escalón del perceptrón, es que esta función es derivable. Esto significa que podemos definir una función de error debido a los pesos $E(\\mathbf{w})$ que podemos minimizar para optimizar el valor de los pesos $\\mathbf{w}$. Normalmente se utiliza el error cuadrático medio:\n",
    "\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_{j=1}^{T}(y_j-a_j)^2$$\n",
    "\n",
    "siendo $E$ el error global, $T$ el núnero total de muestras de entrenamiento, $y_j$ el valor de clase del vector de entrenamiento $j$ (el valor objetivo) y $a_j\\in \\mathbb{R}$ la salida de la función de activación. El término $\\frac{1}{2}$ es por conveniencia al derivar.\n",
    "\n",
    "Uno de los métodos más utilizados en redes neuronales es el denominado **descenso de gradiente**. La lógica tras este método es que al tratar de alcanzar el valor de $\\mathbf{w}$ que minimiza la función de error la pendiente de la función también disminuye:\n",
    "\n",
    "![](images/gradient-descent.png)\n",
    "\n",
    "El gradiente es un vector que apunta en dirección hacia donde crece la pendiente de una superficie. Entonces, para alcanzar el mínimo de la función de error, en este caso $E(\\mathbf{w})$, es necesario \"moverse\" a lo largo de $\\mathbf{w}$ en dirección contraria al gradiente de $E$, esto es:\n",
    "\n",
    "$$\\Delta \\mathbf{w} = - \\eta \\nabla E(\\mathbf{w}) = - \\eta \\left(\\frac{\\partial E(\\mathbf{w})}{\\partial w_0},\\ldots \\frac{\\partial E(\\mathbf{w})}{\\partial w_n} \\right)$$\n",
    "\n",
    "donde\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial w_i}  &= \\frac{\\partial }{\\partial w_i} \\frac{1}{2} \\sum_{j=1}^{T}(y_j-a_j)^2\\\\\n",
    "&= \\frac{1}{2} \\sum_{j=1}^{T}\\frac{\\partial }{\\partial w_i} (y_j-a_j)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{j=1}^{T} 2 (y_j-a_j) \\frac{\\partial }{\\partial w_i} (y_j-a_j) \\\\\n",
    "&= \\sum_{j=1}^{T} (y_j-a_j) \\frac{\\partial }{\\partial w_i} \\left(y_j-\\sum_{\\iota=1}^n w_{\\iota} x_{j\\iota}\\right) \\\\\n",
    "&= \\sum_{j=1}^{T} (y_j-a_j) \\frac{\\partial }{\\partial w_i} \\left(-w_{i} x_{ji}\\right) \\\\\n",
    "&= -\\sum_{j=1}^{T} (y_j-a_j) x_{ji}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "De manera que, para buscar iterativamente el conjunto de pesos que minimiza el error en la neurona Adaline, actualizamos los pesos de acuerdo a la siguiente regla:\n",
    "\n",
    "$$\\Delta \\mathbf{w} = \\eta \\sum_{j=1}^{T} (y_j-a_j) \\mathbf{x_j}$$\n",
    "\n",
    "o bien\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} + \\eta \\sum_{j=1}^{T} (y_j-a_j) \\mathbf{x_j}$$\n",
    "\n",
    "donde el lado izquierdo de la ecuación representa, iterativamente, el nuevo valor de $\\mathbf{w}$, dados los valores en el lado derecho.\n",
    "\n",
    "Si bien esta regla de entrenamiento tiene un aspecto parecido a la regla de entrenamiento para el perceptrón, pueden observarse dos diferencias importantes:\n",
    "1. La primera diferencia es el error se mide a partir de una función continua y no a partir de valores escalón.\n",
    "2. La actualización de los pesos se realiza a partir del error acumulado en todos los vectores de entrenamiento y no vector a vector, como es en el entrenamiento del perceptrón.\n",
    "\n",
    "En los siguientes ejemplos se analiza nuevamente el problema de las flores Iris (con sólo 2 clases), utilizando el clasificador Adaline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vectores mal clasificados de 50 (2.0%)\n"
     ]
    }
   ],
   "source": [
    "# Re-etiquetar las clase de cada vector ejemplo en [-1,1]\n",
    "yAd = np.where(Y == 0, -1, 1)\n",
    "\n",
    "# Normalizar los vectores de características\n",
    "XAd = (X - X.mean()) / X.std()\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "# Número de iteraciones\n",
    "niter = 20\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wAd = np.zeros(XAd.shape[1] + 1)\n",
    "# Number of misclassifications\n",
    "errors = []\n",
    "# Cost function\n",
    "costs = []\n",
    "\n",
    "train_dataAd = XAd[:50]\n",
    "test_dataAd = XAd[50:]\n",
    "train_yAd = yAd[:50]\n",
    "test_yAd = yAd[50:]\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    output = np.dot(train_dataAd, wAd[1:]) + wAd[0]\n",
    "    errors = train_yAd - output\n",
    "    wAd[1:] += eta * train_dataAd.T.dot(errors)\n",
    "    wAd[0] += eta * errors.sum()\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_dataAd, test_yAd) :\n",
    "    activation = np.dot(xi, wAd[1:]) + wAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_dataAd), \n",
    "                                                        errores/len(test_dataAd)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos a continuación la clasificación mediante Adaline sobre los datos de diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 vectores mal clasificados de 130 (76.92307692307693%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[19 61]\n",
      " [39 11]]\n"
     ]
    }
   ],
   "source": [
    "# Re-etiquetar las clase de cada vector ejemplo en [-1,1]\n",
    "df_classAd = np.where(df_class == 0, -1, 1)\n",
    "\n",
    "X_trainPIDAd, X_testPIDAd, y_trainPIDAd, y_testPIDAd = train_test_split(\n",
    "    df_pure.values, df_classAd, test_size=0.33, random_state=0)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Número de iteraciones\n",
    "niter = 200\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPIDAd = np.zeros(X_trainPIDAd.shape[1] + 1)\n",
    "\n",
    "# Number of misclassifications\n",
    "errors = []\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    output = np.dot(X_trainPIDAd, wPIDAd[1:]) + wPIDAd[0]\n",
    "    errors = y_trainPIDAd - output\n",
    "    wPIDAd[1:] += eta * X_trainPIDAd.T.dot(errors)\n",
    "    wPIDAd[0] += eta * errors.sum()\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    activation = np.dot(xi, wPIDAd[1:]) + wPIDAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método descrito anteriormente se conoce también como \"*método de descenso de gradiente por lotes*\" de los pesos (el aprendizaje) se realiza sólo después de acumular los errores para el *lote* completo de los vectores de entrenamiento. Esto es así debido a que es la solución arrojada por el proceso de optimización. Existen otros métodos para optimizar los pesos, siendo la alternativa más directa el llamado \"*método de descenso de gradiente estocástico*\" o de \"*aprendizaje en línea*\". En este método la actualización de los pesos se realiza un vector a al vez (como en el caso del perceptrón). Esta versión suele tener mejor tiempo de convergencia y es útil para mantener la red actualizada con los datos que va recibiendo. Sin embargo, esta a una aproximación al proceso de optimización del error. El resultado es típicamente *oscilatorio* y depende de como se van presentando los ejemplos de entrenamiento. \n",
    "\n",
    "A continuación la aplicación del método de descenso de gradiente estocástico a los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 vectores mal clasificados de 130 (76.92307692307693%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[19 61]\n",
      " [39 11]]\n"
     ]
    }
   ],
   "source": [
    "# Número de iteraciones\n",
    "niter = 20\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPIDAdS = np.zeros(X_trainPIDAd.shape[1] + 1)\n",
    "\n",
    "# Number of misclassifications\n",
    "errors = []\n",
    "# Cost function\n",
    "costs = []\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    for xi, target in zip(X_trainPID, y_trainPID):\n",
    "        activation = np.dot(xi, wPIDAdS[1:]) + wPIDAdS[0]\n",
    "        output = np.where(activation >= 0.0, 1, 0)\n",
    "        error = target - output\n",
    "        wPIDAdS[1:] += eta * xi.dot(error)\n",
    "        wPIDAdS[0] += eta * error\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    activation = np.dot(xi, wPIDAd[1:]) + wPIDAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de activación\n",
    "\n",
    "Otra línea de desarrollo de los modelos neuronales es la selección de la función de activación de la neurona: ni la respuesta escalón ni la respuesta identidad (lineal) son respuestas \"naturales\". Una respuesta escalón implica la \"aparición instantánea\" de una respuesta, pasando de una respuesta nula (o con un valor base) a un nuevo valor (lo cual requiere una cantidad de energía infinita. Una respuesta lineal implicaría que la neurona responde a cualquier estímulo, sin discriminar el ruido y sin permitir que se integren señales pequeñas pero sostenidas para provocar una respuesta. A continuación se muestran un par de gráficas de respuestas de neuronas [VTA](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (tomada de [A-type K+ Current of Dopamine and GABA Neurons in the Ventral Tegmental Area. Susumu Koyama y Sarah B. Appel, Journal of Neurophysiology, Vol. 96(2), 2006, DOI: 10.1152/jn.01318.2005](http://jn.physiology.org/content/96/2/544)), donde se aprecia que la respuesta a la presentación de voltajes de entrada tiene un comportamiento no lineal, tanto para la corriente (imagen a la izquierda) como en la conductancia (derecha). <br><br>\n",
    "\n",
    "![](images/f1.jpg)<br><br>\n",
    "\n",
    "Con el fin de explotar un poco más la metáfora neural y desarrollar modelos más variados se han definido una diversidad de funciones de activación, siendo las principales la función gaussiana, la función sigmoide y la tangente hiperbólica, mostradas a continuación.<br><br>\n",
    "\n",
    "![](images/activation-functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes *feed-forward* y el algoritmo *backpropagation*\n",
    "\n",
    "Por otra parte, en términos de la metáfora neuronal, la función de razonamiento no se logra con una sola neurona, sino con un conglomerado, interconectado, de neuronas:\n",
    "\n",
    "![](images/neuron4.jpg)\n",
    "\n",
    "Una arquitectura que se acerca a la metáfora neuronal es la siguiente:\n",
    "\n",
    "![](images/full_topo.png)\n",
    "\n",
    "En este caso, se trata de una *[red neuronal de impulsos](https://en.wikipedia.org/wiki/Spiking_neural_network)* *[completamente conectada](https://en.wikipedia.org/wiki/Network_topology#Mesh)*. Sin embargo, una opción mucho más común, es el modelo conocido como **multiperceptrón multicapa**. Este nombre es en realidad una generalización sólo válida si las unidades que forman la red son perceptrones, lo cual rara vez es el caso. Un nombre más apropiado es: \"*red neuronal alimentada hacia adelante*\" o red *feed-forward*:\n",
    "\n",
    "![](images/neuron5.png)\n",
    "\n",
    "Una red *Feed-forward* es una malla de neuronas organizadas en una sola dirección, desde una primera *capa de entrada* que recibe todos los estímulos a la entrada de la red (siguiendo la metáfora, estos estímulos pueden ser las señales de las neuronas sensoras) y transfieren estas señales a un conjunto de capas, avanzando hasta la *capa de salida*. La capa de salida arroja la o las salidas de la red neuronal (en la metáfora se trataría, por ejemplo, de las señales hacia las neuronas motoras).\n",
    "\n",
    "Las capas intermedias (entre la capa de entrada y la capa de salida) se denominan *capas ocultas*. Este nombre se debe a que en estas capas desconocemos las entradas y las salidas de cada neurona. \n",
    "\n",
    "#### Algoritmo *backpropagation*\n",
    "\n",
    "El entrenamiento de la red neuronal consiste en ajustar los pesos sinápticos entre las\n",
    "neuronas,  de tal manera que las salidas de las neuronas en la capa de salida\n",
    "correspondan a una cierta salida  prevista, asociada a los datos de entrada. El error al\n",
    "asociar los datos de entrada al valor de salida se calcula individualmente para cada\n",
    "neurona en la capa de salida y se propaga  hacia la primera capa oculta.\n",
    "\n",
    "En este caso no es posible utilizar la regla de Hebb, utilizada para entrenar el perceptrón ni la regla delta utilizada en el modelo Adaline, ya que en ambos casos se requiere conocer, **para cada neurona**, las entradas y las salidas esperadas. Sin embargo, como se anticipó antes, en el caso de la red *feed-forward* solo se conocen las entradas y las salidas **a la red** y no las señales intermedias; de hecho, podemos ver la red neuronal como una *caja negra* en la que es irrelevante conocer las señales internas, siempre que la salida de la red para una entrada específica sea la esperada.\n",
    "\n",
    "\n",
    "![](images/neuron9.png)<br>\n",
    "\n",
    "La salida de la neurona $k$ en la capa de salida (capa $o$) es $y^*_{kj}$ mientras que la salida esperada es $y_{kj}$, de manera que el error en la salida de esta neurona es:\n",
    "\n",
    "$$\\text{error}_k^{(o)} = y_{kj} - y^*_{kj}$$\n",
    "\n",
    "Para las neuronas antecesores, las neuronas de las capas ocultas, no hay valores esperados, por lo que no se puede definir un error por esta vía. Sin embargo, es lógico asumir que el error en las neuronas de salida es generado, parcialmente por los valores de entrada, $x_i^{(o)}$ que recibe y que corresponden a las salidas $z_i^{(L)}$ de las neuronas en la capa previa $L$.\n",
    "\n",
    "En general, la salida de la $k$-ésima neurona en la capa $l$ está dada por\n",
    "\n",
    "$$z^{(l)}_k = g \\left( a^{(l)}_k \\right)$$\n",
    "\n",
    "donde, $g(\\cdot)$ es la función de activación y $a^{(l)}_k$ es la activación de la neurona. \n",
    "\n",
    "$$a^{(l)}_k = \\left(\\sum_{i=1}^{n^{(l-1)}}w^{(l)}_{ki} x^{(l)}_i \\right) - \\theta^{(l)}_k$$\n",
    "\n",
    "siendo $w^{(l)}_{ki}$ el peso sináptico entre la $i$-ésima neurona en la capa $l-1$ y la $k$-ésima neurona en la capa $l$. $n^{(l-1)}$ es el número de neuronas en la capa $l-1$, lo cual corresponde al número de entradas en la cada neurona de la capa $l$. $x^{(l)}_i$ es la $i$-ésima entrada a la capa $l$ y que corresponde a la salida de la neurona $i$ en la capa $l-1$, $z^{(l-1)}_i$ o al rasgo $i$ del vector de entrada si $l$ es la priemra capa oculta. $\\theta^{(l)}_k $ es el valor de bias o valor umbral de la neurona $k$ de la capa $l$. \n",
    "\n",
    "$w^{(l)}_{ki}$ se actualiza de forma iterativa, mediante la ecuación\n",
    "\n",
    "$$w^{(l)}_{ki}(j+1) = w^{(l)}_{ki}(j) + \\Delta w^{(l)}_{ki}(j)$$\n",
    "donde\n",
    "\n",
    "$$\\Delta w^{(l)}_{ki}(j) = \\lambda\\ \\delta^{(l)}_{k}(j)\\ x^{(l-1)}_{i} + \\alpha\\ \\Delta w^{(l)}_{ki}(j-1)$$\n",
    "\n",
    "$\\lambda \\in (0,1)$ es la tasa de aprendizaje y $\\alpha \\in (0,1)$ es el momento. El objetivo del momento es reducir las fluctuaciones en la dirección de cambio de los pesos, al hacer que el cambio en los pesos en el paso actual dependa del cambio previo. $j$ es el paso de iteración, que corresponde a cada presentación de un vector de entrenamiento.\n",
    "\n",
    "$\\delta^{(l)}_{k}$ es la señal de error de la neurona $k$ en la capa $l$ y se define como\n",
    "\n",
    "$$   \\delta^{(l)}_{k}=\\left\\{\n",
    "      \\begin{array}{ll}\n",
    "         g^\\prime\\!\\!\\left(a^{(o)}_k \\right) \\left( y_k - z^{(o)}_k \\right) &\n",
    "                         \\textrm{Para neuronas en la capa de salida} \\\\\n",
    "                         & \\\\\n",
    "         g^\\prime\\!\\!\\left(a^{(l)}_k \\right) \\sum_{m=1}^{n^{(l+1)}} \\delta^{(l+1)}_{m} w^{(l+1)}_{mk} &\n",
    "                         \\textrm{Para neuronas en las capas anteriores a la de salida}\n",
    "      \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Para una función sigmoidea de salida, por ejemplo, tendremos\n",
    "\n",
    "$$\n",
    "g(a) = \\frac{1}{1+e^{-Ga}}\n",
    "$$\n",
    "\n",
    "donde $G$ es la ganancia de la red. La derivada de esta función de activación es:\n",
    "\n",
    "$$g'(a) = G\\ g(a)\\left(1-g(a)\\right)$$\n",
    "\n",
    "A continuación, presentamos los resultados de la red neuronal *feed-forward* entrenada con el método *backpropagation* sobre los datos de prueba. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pima Indians Diabetes Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 vectores mal clasificados de 130 (20.0%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[60 20]\n",
      " [ 6 44]]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1.5e-4, activation='logistic', #identity  tanh\n",
    "                    hidden_layer_sizes=(3,7,2), random_state=1, \n",
    "                    learning_rate_init=0.0001, max_iter=15000)\n",
    "clf.fit(X_trainPIDAd, y_trainPIDAd)                         \n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    output = clf.predict(xi.reshape(1, -1))\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectores mal clasificados de 50 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-4, activation='tanh', #  identity logistic\n",
    "                    hidden_layer_sizes=(3, 2), random_state=1, \n",
    "                    learning_rate_init=0.001, max_iter=500)\n",
    "clf.fit(train_dataAd, train_yAd)                         \n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_dataAd, test_yAd) :\n",
    "    output = clf.predict(xi.reshape(1, -1))\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_yAd), \n",
    "                                                        errores/len(test_yAd)*100))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
