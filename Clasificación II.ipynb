{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Clasificación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Clasificación%20II.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: El método ingenuo de Bayes\n",
    "\n",
    "El método ingenuo de Bayes o *Naive Bayes* describe a una familia de clasificadores que utilizan el Teorema de Bayes para realizar la clasificación de nuevas observaciones de una manera ingenua: asumiendo que existe total independencia entre los diferentes atributos de un objeto. \n",
    "\n",
    "![ ](images/Bayes Sheldon.png)\n",
    "\n",
    "Este es uno de los métodos de clasificación más populares, en parte debido a que funciona *sorprendentemente* bien en muchos problemas y en parte porque ofrece la <u>tranquilidad</u> de \"estar basado\" en una una rama de la matemática bien establecida y con una honorable y sólida reputación... cualquier cosa que esto último signifique :-|.\n",
    "\n",
    "![ ](images/teacher.jpg)\n",
    "\n",
    "\n",
    "### Teorema de Bayes\n",
    "\n",
    "El teorea de Bayes describe la probabilidad de que ocurra un evento en base a las condiciones relacionadas al evento. Matemáticamente:\n",
    "\n",
    "$$\n",
    "P(A\\ |\\ B) = \\frac{P(B\\ |\\ A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Aquí \n",
    "\n",
    "* $A$ y $B$ son dos eventos cualesquiera; $P(A)$ es la probabilidad de que ocurra el evento $A$ y $P(B)$ la probabilidad de que ocurra $B$ sin importar su posible interrelación; asumimos que $P(B)\\neq 0$.<br><br>\n",
    "\n",
    "* $P(A\\ |\\ B)$ es la probabilidad (condicional) de que observemos el evento $A$ sabiendo (por evidencia, por suposición, por presunción o por afirmación) que ha ocurrido el evento $B$. <br><br>\n",
    "\n",
    "* $P(B\\ |\\ A)$ es la probabilidad de $B$ dado $A$.\n",
    "\n",
    "\n",
    "### Teorema de bayes y el problema de clasificación\n",
    "\n",
    "El teorema de Bayes puede emplearse para determinar la clase a la que pertenece un elemento, bajo la siguiente re-escritura:\n",
    "\n",
    "Supongamos que el elemento que queremos clasificar está representado por el vector de características $\\mathbf{x} =(x_{1},\\dots, x_{n})$ y el conjunto de posibles clases es $C = \\{C_1\\ldots, C_k\\ldots, C_K\\}$. Entonces, describimos la probabilidad de que $\\mathbf{x}$ pertenezca a la clase $C_k$ como $p(C_{k}\\ \\mid\\ x_{1},\\dots, x_{n})$. De esta manera, debemos reescribir la ecuación del teorema de Bayes, para este caso como:<br><br>\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(C_{k} \\mid\\mathbf{x}) & = & \\frac{P(\\mathbf{x}\\mid C_{k})P(C_{k})}{P(\\mathbf{x})} \\\\\n",
    "p(C_{k}\\ \\mid\\ x_{1},\\dots, x_{n}) & = & \\frac{P(x_{1},\\dots, x_{n}\\mid C_{k})P(C_{k})}{P(x_{1},\\dots, x_{n})}\n",
    "\\end{eqnarray*}<br><br>\n",
    "\n",
    "Pero (la distribución conjunta para variables condicionalmente dependientes establece)\n",
    "\n",
    "$$\n",
    "P(x_{1},\\dots, x_{n}\\mid C_{k})P(C_{k}) = P(C_{k},x_{1},\\dots ,x_{n})\n",
    "$$\n",
    "\n",
    "Que puede reescribirse como:<br><br>\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(C_{k}, x_{1},\\dots, x_{n}) & = & P(x_{1},\\dots, x_{n}, C_{k})\\\\\n",
    "& = & P(x_{1}\\mid x_{2},\\dots, x_{n}, C_{k})P(x_{2},\\dots, x_{n},C_{k})\\\\\n",
    "& = & P(x_{1}\\mid x_{2},\\dots, x_{n}, C_{k})P(x_{2} \\mid x_{3},\\dots, x_{n}, C_{k}) P(x_{3},\\dots, x_{n}, C_{k}) \\\\\n",
    "& = & \\dots \\\\\n",
    "& = & P(x_{1}\\mid x_{2},\\dots, x_{n}, C_{k}) P(x_{2}\\mid x_{3},\\dots, x_{n}, C_{k})\\dots P(x_{n-1}\\mid x_{n}, C_{k}) P(x_{n}\\mid C_{k}) P(C_{k})\n",
    "\\end{eqnarray*}<br><br>\n",
    "\n",
    "Ahora bien, si asumimos que las características $x_i$ y $x_j$ son condicionalmente independientes, para toda $i\\neq j$, es decir, que la ocurrencia de una característica no está influenciada por la ocurrencia de otra de las características, entonces\n",
    "\n",
    "$$P(x_{i}\\mid x_{i+1},\\dots, x_{n}, C_{k}) = P(x_{i}\\mid C_{k})$$\n",
    "\n",
    "Y podemos reescribir\n",
    "\n",
    "$$P(C_{k} \\mid x_1, \\dots, x_n) = \\frac{P(C_{k}) \\prod_{i=1}^{n} P(x_i \\mid C_{k})} {P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "Por otra parte, $P(x_1, \\dots, x_n)$ es constante una vez definida la entrada, entonces podemos ignorar este término y reescribir la ecuación anterior como:\n",
    "\n",
    "$$P(C_{k} \\mid x_1, \\dots, x_n) \\propto P(C_{k}) \\prod_{i=1}^{n} P(x_i \\mid C_{k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación anterior nos proporciona la *distribución de probabilidades* de que el vector $\\mathbf{x}$ pertenezca a cada una de las clases $C_1\\ldots, C_K$, esto es, el modelo ingenuo de probabilidades de Bayes. Ahora debemos utilizar este modelo para escoger una de las clases. Una opción común es seleccionar aquella clase que es más probable de ajustarse a $\\mathbf{x}$. Esta decisión es lo que se conoce como estimación máxima a posteriori (**MAP**). \n",
    "\n",
    "Es decir, clasificamos $\\mathbf{x}$ dentro de la clase que maximiza el valor de $P(C_{k} \\mid x_1, \\dots, x_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inicializar el ambiente\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from scipy.spatial import distance\n",
    "from sklearn import cluster\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=2, suppress=True) # Cortar la impresión de decimales a 1\n",
    "\n",
    "os.chdir('Data sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenaiento: \n",
      "[[  70.28   42.12]\n",
      " [   0.     56.75]\n",
      " [  79.      2.5 ]\n",
      " [  75.64   11.67]\n",
      " [  82.     58.8 ]\n",
      " [  86.     77.27]\n",
      " [  80.     85.5 ]\n",
      " [  68.2    62.44]\n",
      " [  72.     88.  ]\n",
      " [  74.     80.6 ]\n",
      " [  91.72    0.  ]\n",
      " [  77.69   13.17]\n",
      " [  79.8    71.2 ]\n",
      " [  87.1    75.39]\n",
      " [   0.     80.85]\n",
      " [   0.     59.93]\n",
      " [  96.      5.  ]\n",
      " [  86.     89.58]\n",
      " [   0.     76.25]\n",
      " [   0.     90.62]\n",
      " [ 100.     95.06]\n",
      " [  87.     62.56]\n",
      " [  79.     82.46]\n",
      " [   0.     75.92]\n",
      " [   0.     76.88]\n",
      " [  73.46   83.53]\n",
      " [  92.     87.56]\n",
      " [  77.     84.38]\n",
      " [  70.8    35.  ]\n",
      " [  92.6    66.88]]\n",
      "\n",
      "Dato de prueba:\n",
      "[72, 52]\n",
      "\n",
      "Prototipos de clase (centroides):\n",
      " [[ 80.16  15.64]\n",
      " [  0.    73.89]\n",
      " [ 82.26  78.2 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAE4CAYAAABojpvUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115a939e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Leer los datos de archivo, separar training y test y calcular \"prototipos de clase\"\n",
    "train_set = pd.read_csv(\"datosProm.csv\", names = ['A', 'B']).values\n",
    "test_point = [72, 52]\n",
    "print(\"Datos de entrenaiento: \\n{}\\n\\nDato de prueba:\\n{}\\n\".format(train_set, test_point))\n",
    "\n",
    "from sklearn import cluster\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "num_clusters = 3\n",
    "k_means = cluster.KMeans(n_clusters=num_clusters, init='random')\n",
    "k_means.fit(train_set) \n",
    "print(\"Prototipos de clase (centroides):\\n\", k_means.cluster_centers_)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "colors = ['#ff0000', '#00ff00', '#0000ff']\n",
    "for k in range(num_clusters):\n",
    "    my_members = k_means.labels_ == k\n",
    "    plt.plot(train_set[my_members, 0], train_set[my_members, 1], 'o', \n",
    "             markeredgecolor='k', markerfacecolor=colors[k], markersize=4)\n",
    "    plt.plot(k_means.cluster_centers_[k][0], k_means.cluster_centers_[k][1], 'o', \n",
    "             markeredgecolor='k', markerfacecolor=colors[k], markersize=6)\n",
    "plt.annotate('Punto nuevo', xy=(test_point[0], test_point[1]), xytext=(40, 50),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.1, width=1, headwidth=7))\n",
    "plt.plot(test_point[0], test_point[1], 'w', marker='*', markersize=8)\n",
    "plt.xlim([-10,110])\n",
    "plt.ylim([-10,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos según $k$\n",
    "\n",
    "Existen dos casos generales de implementación del método kNN:\n",
    "#### <big>I</big>. $k=1$ o **Regla del vecino más cercano**. \n",
    "El escenario es muy simple: Buscar el vector de *entrenamiento* $x$ cuya similaridad al vector de entrada $y$ sea mayor (o cuya distancia a $y$ sea menor): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 5 vecinos más próximos son:\n",
      "Vecino 0: 0, dist=10.023673228911644, clase=0, centroide=[ 80.16  15.64]\n",
      "Vecino 1: 7, dist=11.11006750654558, clase=2, centroide=[ 82.26  78.2 ]\n",
      "Vecino 2: 4, dist=12.092973166264777, clase=2, centroide=[ 82.26  78.2 ]\n",
      "Vecino 3: 28, dist=17.04230031421815, clase=0, centroide=[ 80.16  15.64]\n",
      "Vecino 4: 21, dist=18.342004688692022, clase=2, centroide=[ 82.26  78.2 ]\n",
      "\n",
      "El nuevo punto es asignado a la clase 0\n"
     ]
    }
   ],
   "source": [
    "LARGER_DISTANCE = sys.maxsize\n",
    "\n",
    "k_neighs = 5 # 5 vecinos... aunque tomaremos sólo el más cercano\n",
    "neighbors_dists = [LARGER_DISTANCE] * k_neighs\n",
    "neighbors = [0] * k_neighs\n",
    "for i in range(len(train_set)):\n",
    "    dist = distance.euclidean(train_set[i], test_point)\n",
    "    for k in range(k_neighs):\n",
    "        if (dist < neighbors_dists[k]) :\n",
    "            for j in range(k_neighs-1, k, -1):\n",
    "                neighbors_dists[j] = neighbors_dists[j-1]\n",
    "                neighbors[j] = neighbors[j-1] \n",
    "            neighbors_dists[k] = dist\n",
    "            neighbors[k] = i\n",
    "            break\n",
    "            \n",
    "print(\"Los {} vecinos más próximos son:\".format(k_neighs))\n",
    "for k in range(k_neighs):\n",
    "    clase = k_means.labels_[neighbors[k]]\n",
    "    print(\"Vecino {}: {}, dist={}, clase={}, centroide={}\"\n",
    "          .format(k, neighbors[k], neighbors_dists[k], \n",
    "                  clase, k_means.cluster_centers_[clase]))\n",
    "print(\"\\nEl nuevo punto es asignado a la clase\", k_means.labels_[neighbors[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque es demasido simple y suele arrojar malos resultados... a no ser que:\n",
    "\n",
    "0) Que las clases estén muy bien diferenciadas :-S\n",
    "\n",
    "1) El número de ejemplos sea muy grande. En tal caso, es muy probable que para cada nuevo observación haya un elemento en el conjunto de entrenamiento que sea casi idéntico a la nueva obervación.\n",
    "\n",
    "2) Que el vecino más próximo se tome con respecto a los prototipos!... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia del punto de prueba al prototipo de la clase 0: 37.267635896666846\n",
      "Distancia del punto de prueba al prototipo de la clase 1: 75.25305315333033\n",
      "Distancia del punto de prueba al prototipo de la clase 2: 28.1375318758149\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_clusters):\n",
    "    dist = distance.euclidean(k_means.cluster_centers_[i], test_point)\n",
    "    print (\"Distancia del punto de prueba al prototipo de la clase {}: {}\".format(i, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <big>II.</big> $k = K$ ó **Regla de los k-vecinos próximos**. \n",
    "\n",
    "En este caso, se seleccionan los $k$ vectores de entrenamiento más cercanos a la nueva observación. La nueva clase es seleccionada mediante un proceso de *votación*. Existen diversas formas de realizar la votación, ya se un simple conteo de vecinos en cada caso o diversas formas de votación ponderada.\n",
    "\n",
    "En el caso anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votación simple:\n",
      "El nuevo punto es asignado a la clase 2 con 3 vecinos cercanos.\n",
      "\n",
      "Los 5 vecinos más próximos y sus pesos ponderados son:\n",
      "Vecino 0: peso=0.8539057808944022, clase: 0\n",
      "Vecino 1: peso=0.8380716729773634, clase: 2\n",
      "Vecino 2: peso=0.8237459032189313, clase: 2\n",
      "Vecino 3: peso=0.7516098640379241, clase: 0\n",
      "Vecino 4: peso=0.7326667788713791, clase: 2\n",
      "\n",
      "Votación ponderada:\n",
      "El nuevo punto es asignado a la clase 2 con una votación de 2.3944843550676738.\n"
     ]
    }
   ],
   "source": [
    "simple_vote = [0] * num_clusters\n",
    "winner = 0 \n",
    "for k in range(k_neighs):\n",
    "    clase = k_means.labels_[neighbors[k]]\n",
    "    simple_vote[clase] += 1\n",
    "for k in range(num_clusters):\n",
    "    if (simple_vote[k] == max(simple_vote)):\n",
    "        winner = k\n",
    "print(\"Votación simple:\\nEl nuevo punto es asignado a la clase {} con {} vecinos cercanos.\\n\"\n",
    "      .format(winner, simple_vote[winner]))\n",
    "\n",
    "print(\"Los {} vecinos más próximos y sus pesos ponderados son:\".format(k_neighs))\n",
    "suma_dists = sum(neighbors_dists)\n",
    "neighbors_weights = [0] * k_neighs\n",
    "weighted_vote = [0] * num_clusters\n",
    "winner = 0 \n",
    "for k in range(k_neighs):\n",
    "    neighbors_weights[k] = 1 - neighbors_dists[k] / suma_dists\n",
    "    clase = k_means.labels_[neighbors[k]]\n",
    "    weighted_vote[clase] += neighbors_weights[k]\n",
    "    print(\"Vecino {}: peso={}, clase: {}\"\n",
    "          .format(k, neighbors_weights[k], k_means.labels_[neighbors[k]]))\n",
    "for k in range(num_clusters):\n",
    "    if (simple_vote[k] == max(simple_vote)):\n",
    "        winner = k\n",
    "print(\"\\nVotación ponderada:\")\n",
    "print(\"El nuevo punto es asignado a la clase {} con una votación de {}.\"\n",
    "      .format(winner, weighted_vote[winner]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "\n",
    "* El método ingenuo de Bayes suele ser visto como un método más \"*formal*\" que los métodos basados en ejemplos, dado que se basa en un modelo formal, aunque muy limitado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 3px;\">\n",
    "\n",
    "### Tarea 6\n",
    "\n",
    "* Utilice el método de $k$ vecinos próximos para realizar la limpieza de datos en el Pima Indians Diabetes Dataset.\n",
    "\n",
    "**Fecha de entrega**: Martes 27 de septiembre."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
